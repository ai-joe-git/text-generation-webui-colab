{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    %cd /content
    !apt-get -y install -qq aria2
    
    !git clone -b v1.3 https://github.com/camenduru/text-generation-webui
    %cd /content/text-generation-webui
    !pip install -r requirements.txt
    !pip install -U gradio==3.28.3
    
    !mkdir /content/text-generation-webui/repositories
    %cd /content/text-generation-webui/repositories
    !git clone -b v1.2 https://github.com/camenduru/GPTQ-for-LLaMa.git
    %cd GPTQ-for-LLaMa
    !python setup_cuda.py install
    
    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yhyu13/manticore-13b-gptq-4bit/row/main/config.json -d /content/text-generation-webui/models/manticore-13b-gptq-4bit -o config.json
    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yhyu13/manticore-13b-gptq-4bit/raw/main/generation_config.json -d /content/text-generation-webui/models/manticore-13b-gptq-4bit -o generation_config.json
    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yhyu13/manticore-13b-gptq-4bit/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/manticore-13b-gptq-4bit -o special_tokens_map.json
    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yhyu13/manticore-13b-gptq-4bit/resolve/main/tokenizer.model -d /content/text-generation-webui/models/manticore-13b-gptq-4bit -o tokenizer.model
    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yhyu13/manticore-13b-gptq-4bit/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/manticore-13b-gptq-4bit -o tokenizer_config.json
    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yhyu13/manticore-13b-gptq-4bit/resolve/main/4bit-128g.safetensors -d /content/text-generation-webui/models/manticore-13b-gptq-4bit -o 4bit-128g.safetensors
    
    %cd /content/text-generation-webui
    !python server.py --share --chat --wbits 4 --groupsize 128 --model_type llama
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
